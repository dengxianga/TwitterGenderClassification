\section{Experiment Analysis}
In this section, we analyze the results of our experiments of multiple methods for feature extraction, dimension reduction, and classification. 
\subsection{Feature Selection/Extraction}

\subsection{Dimension Reduction}
\subsection{Classification}


The table of approaches and their associated 5-fold cross-validation classification accuracies are shown in the Table \ref{Table 1}

\begin{table}[h!]
\centering
\begin{tabular*}{\textwidth}{c @{\extracolsep{\fill}}ccc}
\hline
\multicolumn{3}{c}{Approach} \\
\cline{1-3}
Feature & Dimension Reduction & Classifier & Cross-Validation Accuracy (\%) \\
\hline
Words + Image features & PCA(500) & Ridge Regression + Sigmoid & $\approx 70\%$\\
Words + Image features & PCA(320) & Ridge Regression + Sigmoid & $\approx 79\%$ \\
Words + Image features & PCA(2000) & Logistic Regression & $\approx 85\%$ \\
Words & None & Logistic Regression & $ 85.96\%$ \\
Words & IG(1000)* & Logistic Regression & $ 85.79\%$ \\
Normalized-Words & None & Naive Bayes & $72.25\%$ \\
Words & None & multinomial Naive Bayes & $ 63.59\%$ \\
Words & IG(100) & multinomial Naive Bayes & $ 77.95\%$ \\
Words & None & Bernoulli Naive Bayes & $ 79.59\%$ \\
Words & IG(350) & Bernoulli Naive Bayes & $ 81.75\%$ \\
Words & IG(350) & Bernoulli Naive Bayes + EM & $ 82.49\%$ \\
Words & PCA(2000) & Artificial Neural Network & $ \approx 86\%$ \\
Words & None & K-Nearest Neighbor (L2) & $ \approx 67\%$ \\
Words & IG(76) & K-Nearest Neighbor (L2) & $ 72.89\%$ \\
Words & IG(84) & K-Nearest Neighbor (Minkowski)  & $ 71.43\%$ \\
words & IG(95) & Random Forest & $83.32\%$ \\
words & IG(95) & Random Forest & $83.32\%$ \\
Words & None & K-means & $ \approx 60\%$\\
Words & IG(1000) & Decision stumps + LogitBoost& $89.11\%$\\
Face-detected Image RGB & PCA(100) & Random Forest & $\approx 69\%$ \\
\hline
\multicolumn{4}{l}{*IG represents Information Gain} \\
\hline
\end{tabular*}
\caption{Experimental results of single classifiers}
\label{Table 1}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{ c c c c}
\hline
\multicolumn{3}{c}{Approach} \\
\cline{1-3}
Feature & Preprocessing & Classifier & Cross-Validation Accuracy (\%) \\
\hline
Words + Image features & IG(1000) for trees & Logistic + Neural Network + Ensemble trees + stacking & $91.11\%$. \\
Words + Image features & IG(1000) for trees & Logistic + Neural Network + Ensemble trees + cascading & $89.04\%$. \\
Words + Image features + Image RGB & IG(1000); Face detection + PCA(100) & Logistic + Neural Network + Ensemble trees + Logistic over PCA-ed RGB + stacking & $\approx 90.37\%$ \\
Words + Image features + Image RGB & IG(1000); face-HOG & Logistic + Neural Network + Ensemble trees + Logistic over HOG + stacking & $\approx 91.55\%$ \\
Words + Image features + Image RGB & IG(1000); face/eyes/nose-HOG & Logistic + Neural Network + Ensemble trees + Logistic over HOG + stacking & $\approx 91.74\%$. 
\end{tabular}
\caption{Experimental results of ensemble classifiers}
\label{Table 2}
\end{table}


