\section{Appendix}

The table of single classifiers and their associated 5-fold cross-validation classification accuracies are shown in the Table \ref{Table 1}. The approaches and 5-fold cross-validation classification accuracies of the ensemble method are shown in the Table \ref{Table 2}.

\begin{table}[h!]
\centering
\begin{tabular*}{\textwidth}{c @{\extracolsep{\fill}}ccc}
\hline
& \multicolumn{2}{c}{Approach}\\
\cline{2-3}
Feature & Dimension Reduction & Classifier & Accuracy (\%) \\
\hline
\hline 
Words + Image features & PCA(500) & Ridge Regression + Sigmoid & $\approx 70\%$\\ \hline
Words + Image features & PCA(320) & Ridge Regression + Sigmoid & $\approx 79\%$ \\ \hline
Words + Image features & PCA(2000) & Logistic Regression & $\approx 85\%$ \\ \hline
Words & None & Logistic Regression & $ 85.96\%$ \\ \hline
Words & IG(1000)* & Logistic Regression & $ 85.79\%$ \\ \hline
Normalized-Words & None & Naive Bayes & $72.25\%$ \\ \hline
Words & IG(100) & multinomial Naive Bayes & $ 77.95\%$ \\ \hline
Words & None & Bernoulli Naive Bayes & $ 79.59\%$ \\ \hline
Words & IG(350) & Bernoulli Naive Bayes + EM & $ 82.49\%$ \\ \hline
Words & PCA(2000) & Artificial Neural Network & $ \approx 86\%$ \\ \hline
Words & IG(76) & K-Nearest Neighbor (L2) & $ 72.89\%$ \\ \hline
Words & IG(84) & K-Nearest Neighbor (Minkowski)  & $ 71.43\%$ \\ \hline
Words & IG(95) & Random Forest & $83.32\%$ \\ \hline
Words & None & K-means & $ \approx 60\%$\\ \hline
Words & IG(1000) & Decision stumps + LogitBoost& $89.11\%$\\ \hline
Face-detected Image RGB & PCA(100) & Random Forest & $\approx 69\%$ \\ \hline
Face-detected Image RGB & Auto-encoder(100) & Logistic Regression & $75.17\%$ \\ \hline
Raw HOG features over &  &  &\\
Face-detected Image RGB & None & Logistic Regression &  $\approx 80\%$  \\  \hline
Raw HOG features (Face/eyes/ & & & \\ 
nose-detected Image RGB) & None & Logistic Regression & $\approx 81\%$ \\ \hline
Raw HOG features (Face/eyes & & & \\ 
/nose-detected Image RGB) & & &\\ 
+ Gaussian Pyramid & None & Logistic Regression & $\approx 82\%$  \\ \hline
Row HOG features (Face/eyes & & &\\
/nose-detected Image RGB) & & & \\ 
+ Gaussian Pyramid & None &  SVM (RBF kernel) & $\approx 83\%$  \\ \hline
HOG features (Face/eyes & & &\\
/nose-detected Image RGB) & & & \\ 
+ Gaussian Pyramid & & &\\
PCA(1500) & None &  SVM (RBF kernel) & $\approx 84\%$  \\ \hline
Dense LBP  & & & \\
(Face-detected Image RGB) & None & SVM (RBF kernel) & $\approx 85\%$  \\ 
\hline
\multicolumn{4}{l}{*IG represents Information Gain} \\
\hline
\end{tabular*}
\caption{Experimental results of single classifiers}
\label{Table 1}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{ c c c}
\hline
\multicolumn{3}{c}{Approach} \\
\cline{1-2}
Preprocessing & Classifier & Accuracy (\%) \\
\hline
\hline
\multicolumn{3}{l}{\textbf{Features: Words + Image features}} \\
IG(1000) on words & Logistic (W) + Neural Network (W) & \\
and image features  & + Ensemble trees (W+I) + stacking & $91.11\%$ \\ \hline
IG(1000) on words & Logistic (W) + Neural Network (W) &  \\
and image features  & + Ensemble trees (W+I) + cascading & $89.04\%$ \\ \hline
\multicolumn{3}{l}{\textbf{Features: Words + Image features + Image RGB}} \\
\hline
IG(1000) on words & Logistic (W) + Neural Network (W) & \\ 
and image features; & + Ensemble trees (W+I) + Logistic (PCA-ed RGB) & \\
Face-detected image RGB PCA(100) & + stacking & $ 90.37\%$ \\ \hline 
IG(1000) on words & Logistic (W) + Neural Network (W) & \\ 
and image features; & + Ensemble trees (W+I) & \\
Face-detected image HOG features & Logistic (HOG) + stacking & $ 91.55\%$ \\ \hline
IG(1000) on words & Logistic (W) + Neural Network (W) & \\ 
and image features; & + Ensemble trees (W+I)+ Logistic (HOG) & \\
Face/eyes/nose-detected & + stacking & \\
image HOG features & & $ 91.74\%$ \\
\hline
IG(1000) on words & Logistic (W) + Neural Network (W) & \\ 
and image features; & + Ensemble trees (W+I) & \\
HOG features (Face/eyes & Logistic (HOG) + stacking &\\
/nose-detected Image RGB) & &  \\ 
Gaussian Pyramid & & $91.9\%$  \\ \hline
IG(1000) on words & Logistic (W) + Neural Network (W) & \\ 
and image features; & + Ensemble trees (W+I) & \\
HOG features (Face/eyes & Logistic (HOG) + stacking (SVM) &\\
/nose-detected Image RGB) & &  \\ 
Gaussian Pyramid + PCA(1500) & & \\
Normalization & & $92.3\%$  \\ \hline
IG(1000) on words & Logistic (W) + Neural Network (W) & \\ 
and image features; & + Ensemble trees (W+I) & \\
HOG features (Face/eyes & Logistic (HOG) + stacking (SVM) &\\
/nose-detected Image RGB) & &  \\ 
Gaussian Pyramid + PCA(1500) & & \\
+ Dense LBP + Normalization & & $92.42\%$  \\ \hline
\multicolumn{3}{l}{*For the classifiers: W: words; I: Image features} \\
\multicolumn{3}{l}{*For the stacking method, we use logistic regression if not specified} \\
\hline
\end{tabular}
\caption{Experimental results of ensemble classifiers}
\label{Table 2}
\end{table}



\begin{thebibliography}{9}

\bibitem{liblinear}
  Rong-En Fan and Kai-Wei Chang and Cho-Jui Hsieh and Xiang-Rui Wang and Chih-Jen Lin,
  \emph{{LIBLINEAR}: A Library for Large Linear Classification},
  Journal of Machine Learning Research 9 (2008), 1871--1874.
\bibitem{libsvm}
    Chang, Chih-Chung and Lin, Chih-Jen, 
    \emph{{LIBSVM}: A library for support vector machines},
    ACM Transactions on Intelligent Systems and Technology, 2:27:1--27:27, 2011.
\bibitem{dltoolbox}
    R. B. Palm,
    \emph{Prediction as a candidate for learning deep hierarchical models of data},
    2012.
\bibitem{boosting}
    Elkan, Charles,
    \emph{Boosting and naive Bayesian learning},
    {Technical Report CS97-557, University of California, San Diego}, 1997.
\bibitem{bagging}
    Oza, Nikunj C,
    \emph{Online bagging and boosting},
    Systems, man and cybernetics, 2005 IEEE international conference, 3:2340 -- 3:2345, IEEE, 2005.

\end{thebibliography}